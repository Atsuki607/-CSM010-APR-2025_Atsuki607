{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "50395787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d578127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "# download Titanic dataset\n",
    "# use train.csv as whole dataset(train and test)\n",
    "data = pd.read_csv(\"./titanic/train.csv\")\n",
    "\n",
    "# check what the dataset looks like\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e66af578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# drop PassengerID (bacause it's just a ID, which doesn't explain each passenger's feature)\n",
    "# drop \"Name\" because it is string data that doesn't contribute to prediction without further processing \n",
    "# drop \"Cabin\" because it has many missing values and is difficult to impute\n",
    "# drop 'Ticket' because it is just random numbers of tickets\n",
    "data = data.drop(['PassengerId','Name','Cabin', 'Ticket'],axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9c8e65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target variable\n",
    "X = data.drop(columns=['Survived'])\n",
    "Y = data['Survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2f6b2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode category values\n",
    "catCols = ['Sex', 'Embarked']\n",
    "\n",
    "# I serched how to use LabelEncoder on ChatGPT\n",
    "le = LabelEncoder()\n",
    "for col in catCols:\n",
    "    X[col] = le.fit_transform(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0ecc5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d497b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values in 'Age' with average value\n",
    "mean_age = X_train['Age'].mean()\n",
    "X_train['Age'] = X_train['Age'].fillna(mean_age)\n",
    "X_test['Age'] = X_test['Age'].fillna(mean_age)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bda96df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out below because this will be contained in pipeline below\n",
    "# test = SelectKBest(score_func=chi2, k=7)\n",
    "# test.fit(X_train, Y_train)\n",
    "\n",
    "# print(test.scores_)\n",
    "# X_train = test.transform(X_train)\n",
    "# X_test = test.transform(X_test)\n",
    "\n",
    "# #I searched how to get names of transformed columns on ChatGPT \n",
    "# selected_columns = X.columns[test.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4395d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "models = {\n",
    "    \"decision tree classifier\": DecisionTreeClassifier(random_state=seed),\n",
    "    \"random forest classifier\": RandomForestClassifier(random_state=seed),\n",
    "    \"logistic regression\": LogisticRegression(random_state=seed),\n",
    "    \"SVM\":SVC(random_state=seed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6756a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree classifier\n",
      "Best score 0.837310924369748\n",
      "Best params: {'classifier__max_depth': 3, 'select__k': 7}\n",
      "random forest classifier\n",
      "Best score 0.8473949579831933\n",
      "Best params: {'classifier__max_depth': 7, 'select__k': 5}\n",
      "logistic regression\n",
      "Best score 0.8121288515406162\n",
      "Best params: {'select__k': 6}\n",
      "SVM\n",
      "Best score 0.8507002801120448\n",
      "Best params: {'select__k': 7}\n"
     ]
    }
   ],
   "source": [
    "# add grid serch for hyper parameter tuning\n",
    "\n",
    "k_vals = np.array([1,2,3,4,5,6,7])\n",
    "depths = np.array([1,2,3,4,5,6,7,8,9,10,None])\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    if model_name in [\"logistic regression\",\"SVM\"]:\n",
    "        pipeline = Pipeline([\n",
    "            ('select', SelectKBest(score_func=chi2)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        param_grid = {\n",
    "            'select__k' : k_vals\n",
    "        }\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('select', SelectKBest(score_func=chi2)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        param_grid = {\n",
    "            'select__k' : k_vals,\n",
    "            'classifier__max_depth' : depths\n",
    "        }\n",
    "    grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5)\n",
    "    grid.fit(X_train,Y_train)\n",
    "\n",
    "    print(\"Best score\", grid.best_score_)\n",
    "    print(\"Best params:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab7a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "388104ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name : decision tree classifier\n",
      "Pclass : 0.09623324305327119\n",
      "Sex : 0.34003122864683594\n",
      "Age : 0.20084448863408566\n",
      "SibSp : 0.06334857788334443\n",
      "Parch : 0.04144702003099539\n",
      "Fare : 0.2435455288802713\n",
      "Embarked : 0.01454991287119608\n",
      "\n",
      "\n",
      "model name : random forest classifier\n",
      "Pclass : 0.07468120681505755\n",
      "Sex : 0.28435953091206606\n",
      "Age : 0.249739957511038\n",
      "SibSp : 0.05494368017033649\n",
      "Parch : 0.04242774393633285\n",
      "Fare : 0.2541714567816247\n",
      "Embarked : 0.03967642387354438\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check feature importances for tree based models\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name == \"decision tree classifier\" or model_name == \"random forest classifier\":\n",
    "        print(f\"model name : {model_name}\")\n",
    "        model.fit(X_train, Y_train)\n",
    "        for feature, importance in zip(selected_columns, model.feature_importances_):\n",
    "            print(f\"{feature} : {importance}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72b952",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2c04680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree classifier\n",
      "confusion matrix \n",
      " [[144  32]\n",
      " [ 53  66]] \n",
      "\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77       176\n",
      "           1       0.67      0.55      0.61       119\n",
      "\n",
      "    accuracy                           0.71       295\n",
      "   macro avg       0.70      0.69      0.69       295\n",
      "weighted avg       0.71      0.71      0.71       295\n",
      "\n",
      "random forest classifier\n",
      "confusion matrix \n",
      " [[146  30]\n",
      " [ 41  78]] \n",
      "\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80       176\n",
      "           1       0.72      0.66      0.69       119\n",
      "\n",
      "    accuracy                           0.76       295\n",
      "   macro avg       0.75      0.74      0.75       295\n",
      "weighted avg       0.76      0.76      0.76       295\n",
      "\n",
      "logistic regression\n",
      "confusion matrix \n",
      " [[148  28]\n",
      " [ 39  80]] \n",
      "\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.82       176\n",
      "           1       0.74      0.67      0.70       119\n",
      "\n",
      "    accuracy                           0.77       295\n",
      "   macro avg       0.77      0.76      0.76       295\n",
      "weighted avg       0.77      0.77      0.77       295\n",
      "\n",
      "SVM\n",
      "confusion matrix \n",
      " [[168   8]\n",
      " [ 90  29]] \n",
      "\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.95      0.77       176\n",
      "           1       0.78      0.24      0.37       119\n",
      "\n",
      "    accuracy                           0.67       295\n",
      "   macro avg       0.72      0.60      0.57       295\n",
      "weighted avg       0.70      0.67      0.61       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for feature, importance in zip(selected_columns, model.feature_importances_):\n",
    "#     print(f\"{feature} : {importance}\")\n",
    "\n",
    "# As a result of the feature_importances_, 'Sex' is the most important feature (0.33)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    if model_name == \"SVM\":\n",
    "        pipeline = Pipeline([\n",
    "            ('select', SelectKBest(score_func=chi2,k=7)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    elif model_name == \"logistic regression\":\n",
    "        pipeline = Pipeline([\n",
    "            ('select', SelectKBest(score_func=chi2, k=6)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    elif model_name == \"decision tree classifier\":\n",
    "        pipeline = Pipeline([\n",
    "            ('select', SelectKBest(score_func=chi2,k=7)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    elif model_name == \"random forest classifier\":\n",
    "            pipeline = Pipeline([\n",
    "            ('select', SelectKBest(score_func=chi2,k=7)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "    print(f\"confusion matrix \\n {cm} \\n\")\n",
    "    print(f\"classification report \\n {report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873bc9f3",
   "metadata": {},
   "source": [
    "###Results###\n",
    "When I used 6 feature, the accuracy scores were below.\n",
    "model name : decision tree classifier\n",
    "\n",
    "classification report \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.74      0.82      0.78       176\n",
    "           1       0.68      0.58      0.63       119\n",
    "\n",
    "    accuracy                           0.72       295\n",
    "   macro avg       0.71      0.70      0.70       295\n",
    "weighted avg       0.72      0.72      0.72       295\n",
    "\n",
    "classification report \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.78      0.85      0.81       176\n",
    "           1       0.74      0.66      0.70       119\n",
    "\n",
    "    accuracy                           0.77       295\n",
    "   macro avg       0.76      0.75      0.76       295\n",
    "weighted avg       0.77      0.77      0.77       295\n",
    "\n",
    "\n",
    "classification report \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.83      0.81       176\n",
    "           1       0.73      0.68      0.70       119\n",
    "\n",
    "    accuracy                           0.77       295\n",
    "   macro avg       0.76      0.76      0.76       295\n",
    "weighted avg       0.77      0.77      0.77       295\n",
    "\n",
    "classification report \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.78      0.86      0.82       176\n",
    "           1       0.75      0.64      0.69       119\n",
    "\n",
    "    accuracy                           0.77       295\n",
    "   macro avg       0.77      0.75      0.75       295\n",
    "weighted avg       0.77      0.77      0.77       295\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
